version: "3.9"
services:
  nginx:
    build: .
    image: heartexlabs/label-studio:latest
    restart: unless-stopped
    ports:
      - "8080:8085"
      - "8081:8086"
    depends_on:
      - app
    environment:
      - LABEL_STUDIO_HOST=${LABEL_STUDIO_HOST:-}
    #   Optional: Specify SSL termination certificate & key
    #   Just drop your cert.pem and cert.key into folder 'deploy/nginx/certs'
    #      - NGINX_SSL_CERT=/certs/cert.pem
    #      - NGINX_SSL_CERT_KEY=/certs/cert.key
    volumes:
      - ./mydata:/label-studio/data:rw
      - ./deploy/nginx/certs:/certs:ro
    #   Optional: Override nginx default conf
    #      - ./deploy/my.conf:/etc/nginx/nginx.conf
    command: nginx

  app:
    stdin_open: true
    tty: true
    build: .
    image: heartexlabs/label-studio:latest
    restart: unless-stopped
    expose:
      - "8000"
    depends_on:
      - db
    environment:
      - DJANGO_DB=default
      - POSTGRE_NAME=postgres
      - POSTGRE_USER=postgres
      - POSTGRE_PASSWORD=
      - POSTGRE_PORT=5432
      - POSTGRE_HOST=db
      - LABEL_STUDIO_HOST=${LABEL_STUDIO_HOST:-}
      - JSON_LOG=1
    #      - LOG_LEVEL=DEBUG
    volumes:
      - ./mydata:/label-studio/data:rw
    command: label-studio-uwsgi

  db:
    image: postgres:11.5
    hostname: db
    restart: unless-stopped
    # Optional: Enable TLS on PostgreSQL
    # Just drop your server.crt and server.key into folder 'deploy/pgsql/certs'
    # NOTE: Both files must have permissions u=rw (0600) or less
    #    command: >
    #      -c ssl=on
    #      -c ssl_cert_file=/var/lib/postgresql/certs/server.crt
    #      -c ssl_key_file=/var/lib/postgresql/certs/server.key
    environment:
      - POSTGRES_HOST_AUTH_METHOD=trust
    volumes:
      - ${POSTGRES_DATA_DIR:-./postgres-data}:/var/lib/postgresql/data
      - ./deploy/pgsql/certs:/var/lib/postgresql/certs:ro

  llm_interactive:
    container_name: llm_interactive
    image: heartexlabs/label-studio-ml-backend:llm-master
    build:
      context: .
      args:
        TEST_ENV: ${TEST_ENV}
    environment:
      - MODEL_DIR=/data/models
      # Specify openai model provider: "openai" or "azure"
      - OPENAI_PROVIDER=openai
      # Specify API key for openai or azure
      - OPENAI_API_KEY=
      # Specify model name for openai or azure (by default it uses "gpt-3.5-turbo")
      - OPENAI_MODEL=gpt-3.5-turbo
      # Internal prompt template for the model is:
      # **Source Text**:\n\n"{text}"\n\n**Task Directive**:\n\n"{prompt}"
      # if you want to specify task data keys in the prompt (i.e. input <TextArea name="$PROMPT_PREFIX..."/>, set this to 0
      - USE_INTERNAL_PROMPT_TEMPLATE=0
      # You can define the default prompt to be used before the user input
      # Can be the path to the file with the prompt or the prompt itself
      # ! Note that USE_INTERNAL_PROMPT_TEMPLATE should be set to 0 in this case
      - DEFAULT_PROMPT=default_prompt.txt
      # Prompt prefix for the TextArea component in the frontend to be used for the user input
      - PROMPT_PREFIX=prompt
      # Log level for the server
      - LOG_LEVEL=DEBUG
      # Number of responses to generate for each request
      - NUM_RESPONSES=1
      # Temperature for the model
      - TEMPERATURE=0.7
      # Azure resourse endpoint (in case OPENAI_PROVIDER=azure)
      - AZURE_RESOURCE_ENDPOINT=
      # Azure deployment name (in case OPENAI_PROVIDER=azure)
      - AZURE_DEPLOYMENT_NAME=
      # Azure API version (in case OPENAI_PROVIDER=azure)
      - AZURE_API_VERSION=2023-05-15
      # specify these parameters if you want to use basic auth for the model server
      - BASIC_AUTH_USER=
      - BASIC_AUTH_PASS=
    ports:
      - 9090:9090
    volumes:
      - "./data/server:/data"