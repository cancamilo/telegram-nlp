{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c32edaea-670d-415b-950d-8c08fa15968a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sagemaker>=2.140.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.184.0)\n",
      "Requirement already satisfied: transformers==4.26.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.26.1)\n",
      "Requirement already satisfied: datasets[s3]==2.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.10.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.26.1) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.26.1) (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.26.1) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.26.1) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.26.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.26.1) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.26.1) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.26.1) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.26.1) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (0.18.0)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (0.4.2)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker>=2.140.0) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker>=2.140.0) (1.28.33)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker>=2.140.0) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker>=2.140.0) (4.23.4)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker>=2.140.0) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker>=2.140.0) (6.8.0)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.7.5)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker>=2.140.0) (4.18.4)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker>=2.140.0) (3.9.1)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker>=2.140.0) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.33 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (1.31.33)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.10.1) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.10.1) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.10.1) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.140.0) (3.16.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.26.1) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.26.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.26.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.26.1) (2023.5.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-pasta->sagemaker>=2.140.0) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.140.0) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.140.0) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.140.0) (0.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets[s3]==2.10.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets[s3]==2.10.1) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker>=2.140.0) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker>=2.140.0) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from schema->sagemaker>=2.140.0) (21.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]==2.10.1\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a17a62-1994-4ada-8376-5139f292ab96",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sage maker config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e298eb96-fad1-4d1c-babe-4a6221d9f37a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc5be5-ea6b-4b83-8f9d-e5923b3165ec",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a082ac09-bf67-42e2-a793-292b74c29e90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def score_sentiment_multiclass(x):\n",
    "  \"\"\"\n",
    "  Transform the text labels into codes.\n",
    "  \"\"\"\n",
    "  if x == \"positive\":\n",
    "    return 0\n",
    "\n",
    "  if x == \"negative\":\n",
    "    return 1\n",
    "\n",
    "  if x == \"neutral\":\n",
    "    return 2\n",
    "  \n",
    "  return 0\n",
    "\n",
    "bucket = 'my-nlp-datalake'\n",
    "\n",
    "data_key = 'pos_labeled_data.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "df_pred = pd.read_csv(data_location)\n",
    "\n",
    "df_pred[\"target\"] = df_pred[\"sentiment\"].apply(score_sentiment_multiclass)\n",
    "\n",
    "n_samples = 10000\n",
    "\n",
    "# load Dataset from Pandas DataFrame\n",
    "dataset = Dataset.from_pandas(df_pred[[\"text\", \"target\", \"sentiment\", \"id\"]].iloc[:n_samples])\n",
    "ds = dataset.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21f67315-ff2d-4c9c-9d8c-eb0e5c5b49f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# create tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"clean_message\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = ds.map(tokenize, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\", \"sentiment\", \"id\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"target\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# tokenize train and test datasets\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "test_dataset = tokenized_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b25b6e17-c97a-49c5-98f3-fb46c6e88542",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 7500\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52800c6d-481b-4f67-8b2d-c49f6777d191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'torch',\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['input_ids', 'attention_mask', 'labels'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81914ef7-4853-4516-b349-a2eb7a289823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-eu-central-1-464909088200'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4cc1f7c8-b504-4f00-96b4-87b3dc189ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "s3_prefix = \"datasets/telegram_sentiment\"\n",
    "\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1bd6480b-6161-46e7-9265-80a7bded7ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# training job\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters={\n",
    "    \"epochs\": 1,                            # number of training epochs\n",
    "    \"train_batch_size\": 32,                 # training batch size\n",
    "    \"model_name\": model_checkpoint  # name of pretrained model\n",
    "}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",                 # fine-tuning script to use in training job\n",
    "    source_dir=\"./scripts\",                 # directory where fine-tuning script is stored\n",
    "    instance_type=\"ml.p3.2xlarge\",          # instance type\n",
    "    instance_count=1,                       # number of instances\n",
    "    role=role,                              # IAM role used in training job to acccess AWS resources (S3)\n",
    "    transformers_version=\"4.26\",             # Transformers version\n",
    "    pytorch_version=\"1.13\",                  # PyTorch version\n",
    "    py_version=\"py39\",                      # Python version\n",
    "    hyperparameters=hyperparameters         # hyperparameters to use in training job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c09a728f-17e4-4f4d-8f55-e66e45f685e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-09-11-16-52-44-390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-09-11 16:52:44 Starting - Starting the training job...\n",
      "2023-09-11 16:53:11 Starting - Preparing the instances for training......\n",
      "2023-09-11 16:54:09 Downloading - Downloading input data...\n",
      "2023-09-11 16:54:29 Training - Downloading the training image........................\n",
      "2023-09-11 16:58:20 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:38,125 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:38,147 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:38,160 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:38,163 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:38,456 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:38,492 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:38,528 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:38,542 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-09-11-16-52-44-390\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-464909088200/huggingface-pytorch-training-2023-09-11-16-52-44-390/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-464909088200/huggingface-pytorch-training-2023-09-11-16-52-44-390/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-09-11-16-52-44-390\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-464909088200/huggingface-pytorch-training-2023-09-11-16-52-44-390/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m[2023-09-11 16:58:40.840: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:40,848 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:40,881 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:44,574 - __main__ - INFO -  loaded train_dataset length is: 7500\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:44,574 - __main__ - INFO -  loaded test_dataset length is: 2500\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 51.6kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  12%|█▏        | 31.5M/268M [00:00<00:00, 283MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  27%|██▋       | 73.4M/268M [00:00<00:00, 345MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  43%|████▎     | 115M/268M [00:00<00:00, 369MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  59%|█████▊    | 157M/268M [00:00<00:00, 368MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  74%|███████▍  | 199M/268M [00:00<00:00, 369MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  90%|█████████ | 241M/268M [00:00<00:00, 359MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:00<00:00, 351MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 7.33kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.27MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.27MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 38.6MB/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 7500\u001b[0m\n",
      "\u001b[34mNum examples = 7500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mNum Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 235\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 235\n",
      "  Number of trainable parameters = 66955779\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66955779\u001b[0m\n",
      "\u001b[34m0%|          | 0/235 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-09-11 16:58:49.938: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-09-11 16:58:49,946 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-09-11 16:58:49.983 algo-1:49 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-09-11 16:58:50.023 algo-1:49 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-09-11 16:58:50.024 algo-1:49 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-09-11 16:58:50.024 algo-1:49 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-09-11 16:58:50.025 algo-1:49 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-09-11 16:58:50.025 algo-1:49 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m0%|          | 1/235 [00:01<06:42,  1.72s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/235 [00:02<03:47,  1.02it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 3/235 [00:02<02:51,  1.35it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 4/235 [00:03<02:24,  1.60it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 5/235 [00:03<02:09,  1.77it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 6/235 [00:03<02:00,  1.90it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 7/235 [00:04<01:54,  1.99it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 8/235 [00:04<01:50,  2.05it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 9/235 [00:05<01:47,  2.09it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 10/235 [00:05<01:46,  2.12it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 11/235 [00:06<01:44,  2.14it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 12/235 [00:06<01:43,  2.15it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 13/235 [00:07<01:42,  2.16it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 14/235 [00:07<01:42,  2.16it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 15/235 [00:08<01:41,  2.17it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 16/235 [00:08<01:40,  2.18it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 17/235 [00:09<01:39,  2.18it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 18/235 [00:09<01:39,  2.19it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 19/235 [00:09<01:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 20/235 [00:10<01:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 21/235 [00:10<01:37,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 22/235 [00:11<01:37,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 23/235 [00:11<01:37,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 24/235 [00:12<01:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 25/235 [00:12<01:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 26/235 [00:13<01:36,  2.17it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 27/235 [00:13<01:35,  2.17it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 28/235 [00:14<01:35,  2.17it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 29/235 [00:14<01:34,  2.17it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 30/235 [00:14<01:34,  2.18it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 31/235 [00:15<01:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 32/235 [00:15<01:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 33/235 [00:16<01:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 34/235 [00:16<01:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 35/235 [00:17<01:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 36/235 [00:17<01:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 37/235 [00:18<01:30,  2.18it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 38/235 [00:18<01:30,  2.18it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 39/235 [00:19<01:29,  2.19it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 40/235 [00:19<01:29,  2.19it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 41/235 [00:20<01:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 42/235 [00:20<01:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 43/235 [00:20<01:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 44/235 [00:21<01:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 45/235 [00:21<01:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 46/235 [00:22<01:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 47/235 [00:22<01:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 48/235 [00:23<01:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 49/235 [00:23<01:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 50/235 [00:24<01:24,  2.18it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 51/235 [00:24<01:24,  2.19it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 52/235 [00:25<01:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 53/235 [00:25<01:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 54/235 [00:25<01:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 55/235 [00:26<01:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 56/235 [00:26<01:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 57/235 [00:27<01:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 58/235 [00:27<01:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 59/235 [00:28<01:20,  2.18it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 60/235 [00:28<01:20,  2.18it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 61/235 [00:29<01:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 62/235 [00:29<01:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 63/235 [00:30<01:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 64/235 [00:30<01:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 65/235 [00:31<01:17,  2.19it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 66/235 [00:31<01:16,  2.20it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 67/235 [00:31<01:16,  2.20it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 68/235 [00:32<01:15,  2.20it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 69/235 [00:32<01:15,  2.19it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 70/235 [00:33<01:15,  2.19it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 71/235 [00:33<01:14,  2.20it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 72/235 [00:34<01:14,  2.20it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 73/235 [00:34<01:13,  2.20it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 74/235 [00:35<01:13,  2.20it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 75/235 [00:35<01:12,  2.20it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 76/235 [00:36<01:12,  2.20it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 77/235 [00:36<01:11,  2.20it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 78/235 [00:36<01:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 79/235 [00:37<01:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 80/235 [00:37<01:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 81/235 [00:38<01:10,  2.19it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 82/235 [00:38<01:09,  2.19it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 83/235 [00:39<01:09,  2.19it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 84/235 [00:39<01:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 85/235 [00:40<01:08,  2.19it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 86/235 [00:40<01:08,  2.19it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 87/235 [00:41<01:07,  2.19it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 88/235 [00:41<01:07,  2.19it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 89/235 [00:41<01:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 90/235 [00:42<01:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 91/235 [00:42<01:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 92/235 [00:43<01:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 93/235 [00:43<01:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 94/235 [00:44<01:04,  2.18it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 95/235 [00:44<01:04,  2.18it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 96/235 [00:45<01:03,  2.18it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 97/235 [00:45<01:03,  2.18it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 98/235 [00:46<01:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 99/235 [00:46<01:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 100/235 [00:47<01:01,  2.18it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 101/235 [00:47<01:01,  2.18it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 102/235 [00:47<01:00,  2.18it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 103/235 [00:48<01:00,  2.19it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 104/235 [00:48<00:59,  2.19it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 105/235 [00:49<00:59,  2.19it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 106/235 [00:49<00:58,  2.19it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 107/235 [00:50<00:58,  2.19it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 108/235 [00:50<00:58,  2.19it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 109/235 [00:51<00:57,  2.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 110/235 [00:51<00:57,  2.18it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 111/235 [00:52<00:56,  2.18it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 112/235 [00:52<00:56,  2.18it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 113/235 [00:52<00:56,  2.18it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 114/235 [00:53<00:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 115/235 [00:53<00:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 116/235 [00:54<00:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 117/235 [00:54<00:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 118/235 [00:55<00:53,  2.18it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 119/235 [00:55<00:53,  2.17it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 120/235 [00:56<00:52,  2.17it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 121/235 [00:56<00:52,  2.18it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 122/235 [00:57<00:51,  2.18it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 123/235 [00:57<00:51,  2.18it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 124/235 [00:58<00:51,  2.17it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 125/235 [00:58<00:50,  2.17it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 126/235 [00:58<00:50,  2.16it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 127/235 [00:59<00:49,  2.17it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 128/235 [00:59<00:49,  2.17it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 129/235 [01:00<00:48,  2.18it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 130/235 [01:00<00:48,  2.18it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 131/235 [01:01<00:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 132/235 [01:01<00:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 133/235 [01:02<00:46,  2.18it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 134/235 [01:02<00:46,  2.19it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 135/235 [01:03<00:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 136/235 [01:03<00:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 137/235 [01:03<00:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 138/235 [01:04<00:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 139/235 [01:04<00:43,  2.19it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 140/235 [01:05<00:43,  2.19it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 141/235 [01:05<00:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 142/235 [01:06<00:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 143/235 [01:06<00:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 144/235 [01:07<00:41,  2.19it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 145/235 [01:07<00:41,  2.19it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 146/235 [01:08<00:40,  2.19it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 147/235 [01:08<00:40,  2.20it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 148/235 [01:09<00:39,  2.20it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 149/235 [01:09<00:39,  2.20it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 150/235 [01:09<00:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 151/235 [01:10<00:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 152/235 [01:10<00:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 153/235 [01:11<00:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 154/235 [01:11<00:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 155/235 [01:12<00:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 156/235 [01:12<00:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 157/235 [01:13<00:35,  2.18it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 158/235 [01:13<00:35,  2.17it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 159/235 [01:14<00:34,  2.18it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 160/235 [01:14<00:34,  2.18it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 161/235 [01:14<00:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 162/235 [01:15<00:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 163/235 [01:15<00:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 164/235 [01:16<00:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 165/235 [01:16<00:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 166/235 [01:17<00:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 167/235 [01:17<00:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 168/235 [01:18<00:30,  2.18it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 169/235 [01:18<00:30,  2.17it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 170/235 [01:19<00:29,  2.17it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 171/235 [01:19<00:29,  2.17it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 172/235 [01:20<00:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 173/235 [01:20<00:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 174/235 [01:20<00:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 175/235 [01:21<00:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 176/235 [01:21<00:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 177/235 [01:22<00:26,  2.17it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 178/235 [01:22<00:26,  2.17it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 179/235 [01:23<00:25,  2.17it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 180/235 [01:23<00:25,  2.17it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 181/235 [01:24<00:24,  2.17it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 182/235 [01:24<00:24,  2.17it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 183/235 [01:25<00:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 184/235 [01:25<00:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 185/235 [01:25<00:22,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 186/235 [01:26<00:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 187/235 [01:26<00:22,  2.17it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 188/235 [01:27<00:21,  2.17it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 189/235 [01:27<00:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 190/235 [01:28<00:20,  2.18it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 191/235 [01:28<00:20,  2.18it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 192/235 [01:29<00:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 193/235 [01:29<00:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 194/235 [01:30<00:18,  2.18it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 195/235 [01:30<00:18,  2.18it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 196/235 [01:31<00:17,  2.18it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 197/235 [01:31<00:17,  2.18it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 198/235 [01:31<00:16,  2.18it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 199/235 [01:32<00:16,  2.17it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 200/235 [01:32<00:16,  2.17it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 201/235 [01:33<00:15,  2.17it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 202/235 [01:33<00:15,  2.18it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 203/235 [01:34<00:14,  2.18it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 204/235 [01:34<00:14,  2.18it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 205/235 [01:35<00:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 206/235 [01:35<00:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 207/235 [01:36<00:12,  2.17it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 208/235 [01:36<00:12,  2.17it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 209/235 [01:37<00:11,  2.17it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 210/235 [01:37<00:11,  2.17it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 211/235 [01:37<00:11,  2.18it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 212/235 [01:38<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 213/235 [01:38<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 214/235 [01:39<00:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 215/235 [01:39<00:09,  2.19it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 216/235 [01:40<00:08,  2.19it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 217/235 [01:40<00:08,  2.19it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 218/235 [01:41<00:07,  2.19it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 219/235 [01:41<00:07,  2.18it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 220/235 [01:42<00:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 221/235 [01:42<00:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 222/235 [01:42<00:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 223/235 [01:43<00:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 224/235 [01:43<00:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 225/235 [01:44<00:04,  2.18it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 226/235 [01:44<00:04,  2.18it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 227/235 [01:45<00:03,  2.18it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 228/235 [01:45<00:03,  2.19it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 229/235 [01:46<00:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 230/235 [01:46<00:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 231/235 [01:47<00:01,  2.18it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 232/235 [01:47<00:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 233/235 [01:48<00:00,  2.19it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 234/235 [01:48<00:00,  2.18it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 235/235 [01:48<00:00,  2.65it/s]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 2500\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mNum examples = 2500\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/40 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 2/40 [00:00<00:05,  6.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 3/40 [00:00<00:08,  4.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m10%|█         | 4/40 [00:00<00:08,  4.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▎        | 5/40 [00:01<00:09,  3.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 6/40 [00:01<00:09,  3.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 7/40 [00:01<00:09,  3.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 8/40 [00:02<00:09,  3.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▎       | 9/40 [00:02<00:09,  3.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 10/40 [00:02<00:08,  3.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 11/40 [00:03<00:08,  3.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 12/40 [00:03<00:08,  3.31it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▎      | 13/40 [00:03<00:08,  3.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 14/40 [00:03<00:07,  3.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 15/40 [00:04<00:07,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 16/40 [00:04<00:07,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▎     | 17/40 [00:04<00:07,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 18/40 [00:05<00:06,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 19/40 [00:05<00:06,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 20/40 [00:05<00:06,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▎    | 21/40 [00:06<00:05,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 22/40 [00:06<00:05,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▊    | 23/40 [00:06<00:05,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 24/40 [00:07<00:04,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 25/40 [00:07<00:04,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 26/40 [00:07<00:04,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 27/40 [00:07<00:03,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 28/40 [00:08<00:03,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▎  | 29/40 [00:08<00:03,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 30/40 [00:08<00:03,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 31/40 [00:09<00:02,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 32/40 [00:09<00:02,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▎ | 33/40 [00:09<00:02,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 34/40 [00:10<00:01,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 35/40 [00:10<00:01,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 36/40 [00:10<00:01,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▎| 37/40 [00:10<00:00,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 38/40 [00:11<00:00,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 39/40 [00:11<00:00,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6288491487503052, 'eval_accuracy': 0.7452, 'eval_f1': 0.7445358561122599, 'eval_precision': 0.7462997140877272, 'eval_recall': 0.7452, 'eval_runtime': 11.9437, 'eval_samples_per_second': 209.315, 'eval_steps_per_second': 3.349, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 235/235 [02:00<00:00,  2.65it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 40/40 [00:11<00:00,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 120.6152, 'train_samples_per_second': 62.181, 'train_steps_per_second': 1.948, 'train_loss': 0.8653986504737367, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 235/235 [02:00<00:00,  2.65it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 235/235 [02:00<00:00,  1.95it/s]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 2500\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mNum examples = 2500\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/40 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 2/40 [00:00<00:05,  6.52it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 3/40 [00:00<00:08,  4.61it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 4/40 [00:00<00:09,  3.99it/s]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 5/40 [00:01<00:09,  3.71it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 6/40 [00:01<00:09,  3.55it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 7/40 [00:01<00:09,  3.46it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 8/40 [00:02<00:09,  3.40it/s]\u001b[0m\n",
      "\u001b[34m22%|██▎       | 9/40 [00:02<00:09,  3.35it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 10/40 [00:02<00:09,  3.33it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 11/40 [00:03<00:08,  3.30it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 12/40 [00:03<00:08,  3.29it/s]\u001b[0m\n",
      "\u001b[34m32%|███▎      | 13/40 [00:03<00:08,  3.28it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 14/40 [00:03<00:07,  3.27it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 15/40 [00:04<00:07,  3.27it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 16/40 [00:04<00:07,  3.26it/s]\u001b[0m\n",
      "\u001b[34m42%|████▎     | 17/40 [00:04<00:07,  3.26it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 18/40 [00:05<00:06,  3.27it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 19/40 [00:05<00:06,  3.27it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 20/40 [00:05<00:06,  3.26it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▎    | 21/40 [00:06<00:05,  3.26it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 22/40 [00:06<00:05,  3.26it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▊    | 23/40 [00:06<00:05,  3.26it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 24/40 [00:07<00:04,  3.26it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 25/40 [00:07<00:04,  3.26it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 26/40 [00:07<00:04,  3.27it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 27/40 [00:07<00:03,  3.27it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 28/40 [00:08<00:03,  3.26it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▎  | 29/40 [00:08<00:03,  3.27it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 30/40 [00:08<00:03,  3.27it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 31/40 [00:09<00:02,  3.26it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 32/40 [00:09<00:02,  3.26it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▎ | 33/40 [00:09<00:02,  3.26it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 34/40 [00:10<00:01,  3.26it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 35/40 [00:10<00:01,  3.26it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 36/40 [00:10<00:01,  3.25it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▎| 37/40 [00:11<00:00,  3.25it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 38/40 [00:11<00:00,  3.25it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 39/40 [00:11<00:00,  3.26it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 40/40 [00:11<00:00,  3.43it/s]\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2023-09-11 17:01:03,845 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-11 17:01:03,845 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-11 17:01:03,846 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-09-11 17:01:11 Uploading - Uploading generated training model\n",
      "2023-09-11 17:01:37 Completed - Training job completed\n",
      "Training seconds: 449\n",
      "Billable seconds: 449\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
