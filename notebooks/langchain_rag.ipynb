{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Demo with telegram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load telegram messages\n",
    "channel_id = \"singularitynet\"\n",
    "df = pd.read_csv(f\"data/{channel_id}.csv\")\n",
    "              \n",
    "# Use langchain wrapper to load dataframe              \n",
    "df_loader = DataFrameLoader(df, page_content_column=\"message\")\n",
    "docs = df_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    âš ï¸ Safety Warning â€¼ï¸ \\n\\nâ€¼ï¸ The ASI token is n...\n",
       "1    âš ï¸WARNING!!!âš ï¸ ðŸš¨SAFETY ALERT!!!ðŸš¨\\n\\nNeither Si...\n",
       "2                                              !safety\n",
       "3    We always include a warning, see last sentence...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"message\"].iloc[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_id = \"singularitynet\"\n",
    "df = pd.read_csv(f\"data/{channel_id}.csv\")\n",
    "df_loader = DataFrameLoader(df, page_content_column=\"message\")\n",
    "docs = df_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to delete  Collection openai_embeddings_ingularitynet does not exist.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "\n",
    "CLEAR_COLLECTION = True\n",
    "\n",
    "if CLEAR_COLLECTION:\n",
    "    try:\n",
    "        persistent_client.delete_collection(f\"openai_embeddings_{channel_id[1:]}\")\n",
    "    except Exception as e:\n",
    "        print(\"unable to delete \", e)\n",
    "\n",
    "# How to create a client with reset allowed\n",
    "# client = chromadb.HttpClient(settings=Settings(allow_reset=True))\n",
    "# client.reset()  # resets the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our loaded collection of telegram messages, it is time to create embeddings using openAI. Chroma db offers us a simple way to achieve this.\n",
    "\n",
    "Beware! This code performs several request to the openai API in order to create embeddings for each of each message in `docs`. Depending on the size of your dataset, this could incurre in high costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camiloramirezf/Library/Caches/pypoetry/virtualenvs/telegram-nlp-cUHCjj8z-py3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "openai_embeddings_function = OpenAIEmbeddings()\n",
    "\n",
    "openai_chroma_client = Chroma.from_documents(\n",
    "    docs, \n",
    "    openai_embeddings_function, \n",
    "    client=persistent_client, \n",
    "    collection_name = f\"openai_embeddings_{channel_id[1:]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to perform similarity search\n",
    "def search_db(db_client, query: str, top_k = 100):\n",
    "    docs = db_client.similarity_search(query, k=top_k)\n",
    "\n",
    "    # print results\n",
    "    for item in docs[:5]:\n",
    "        print(f\"\\n{item.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings are saved locally. Now we can perform search queries by obtaining the most similar documents to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- in addition to securing the network you also provide a real world case for solving problems  eg genome sequencing, graphics rendering, etc...\n",
      "- upto you to find and research\n",
      "- Can you please write in simple words the usecases of flux coin ? Services which requires flux token ?\n",
      "- What are the $FLUX usecases?Let's find out httpstwitter.comHouseofChimerastatus1671902756808818690\n",
      "- It may help to \n",
      "- Harness the power of decentralisation Explore RunOnFlux use cases Let's read onhttpstwitter.comHouseofChimerastatus1709901477689364513\n",
      "- cycle through the different options and see what the issue could be\n",
      "- Usage is one of the key things that separates Flux from its competitors, show the below some love on twitter and help spread the word httpstwitter.comcryptoviumstatus1686329414940823552?s20\n",
      "- Anyone can apply to use their services for payments\n",
      "- bring the network to life\n"
     ]
    }
   ],
   "source": [
    "query = \"use cases\"\n",
    "search_db(openai_chroma_client, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=openai_chroma_client.as_retriever(search_type=\"mmr\", search_kwargs={'fetch_k': 30}), \n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are some use cases of flux?\"\n",
    "response = qa({\"query\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Some potential use cases for Flux could include:\\n\\n1. Real-time data processing and analytics: Flux is designed to handle large volumes of data and perform computations in real-time, making it suitable for applications that require fast data processing and analytics.\\n\\n2. Internet of Things (IoT) applications: Flux can be used to collect and process data from various IoT devices, enabling the creation of smart and connected systems.\\n\\n3. Financial services: Flux's real-time capabilities make it well-suited for financial applications, such as high-frequency trading, fraud detection, and risk analysis.\\n\\n4. E-commerce and recommendation systems: Flux can be used to process and analyze user data in real-time, allowing for personalized recommendations and targeted advertising.\\n\\n5. Social media analytics: Flux's ability to handle streaming data makes it a good choice for analyzing social media feeds in real-time, enabling sentiment analysis, trend detection, and social network analysis.\\n\\nThese are just a few examples, and the use cases for Flux can vary depending on the specific requirements of an application.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing persisted chroma collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3117"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the existing collection has documents\n",
    "channel_id = \"@runonflux\"\n",
    "collection_name = f\"openai_embeddings_{channel_id[1:]}\"\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_collection(collection_name)\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "\n",
    "openai_embeddings_function = OpenAIEmbeddings()\n",
    "\n",
    "openai_client = Chroma(\n",
    "    persist_directory=\"./chroma\", \n",
    "    collection_name=collection_name, \n",
    "    embedding_function=openai_embeddings_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- People love to Fud and create unneeded distractions\n",
      "- I hope so too and we all hope so. But let's say that seeing your deleted message doesn't do much pleasure. And I certainly didn't take the liberty of making fud.\n",
      "- lets hope for the best\n",
      "- it's up to date mate \n",
      "- I think that now come dump. \n",
      "- chaos trouble violence disorder destruction confusion havocfracas commotion\n",
      "- reddug akhtarg need to be more vigilant with fake admin tagging\n",
      "- reddug could I do you ?\n",
      "- Ill give it another go\n",
      "- this is probably the only thing holding the project back\n"
     ]
    }
   ],
   "source": [
    "query = \"fud\"\n",
    "docs = openai_client.similarity_search(query, k=20)\n",
    "# print results\n",
    "for item in docs[:10]:\n",
    "    print(f\"- {item.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "template= \"\"\"\n",
    "The context given are several messages retrieved from a chat about a blockchain project. Based on those messages try to answer the user question.\n",
    "----------------\n",
    "context: {context}\n",
    "user question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=openai_client.as_retriever(search_type=\"mmr\", search_kwargs={'fetch_k': 100}), \n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] template='\\nThe context given are several messages retrieved from a chat about a blockchain project. Based on those messages try to answer the user question.\\n----------------\\ncontext: {context}\\nuser question: {question}\\n'\n"
     ]
    }
   ],
   "source": [
    "print(qa.combine_documents_chain.llm_chain.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"What makes this project unique?\"\n",
    "response = qa({\"query\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the given context, it seems that the project mentioned (Flux) is being compared to other blockchain projects. One user mentions that the project is \"probably the only thing holding the project back.\" Another user asks what features make Flux stand out compared to other chains.\\n\\nHowever, there is not enough information provided in the given messages to determine what specifically makes Flux unique. Further details or messages are needed to answer the user\\'s question accurately.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"result\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram-nlp-6m6KizWy-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
