{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Demo with telegram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilo.ramirez/Library/Caches/pypoetry/virtualenvs/telegram-nlp-6m6KizWy-py3.10/lib/python3.10/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using existing Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index initialization\n",
    "from semantic_search_generator import SemanticSearchGenerator\n",
    "\n",
    "channel_id = \"@runonflux\"\n",
    "model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "INDEX_NAME = \"telegram-embeddings\"\n",
    "\n",
    "generator = SemanticSearchGenerator(model_name)\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(\n",
    "    api_key=os.environ[\"PINECONE_APIKEY\"],\n",
    "    environment=\"us-west1-gcp\"\n",
    ")\n",
    "\n",
    "# connect to index\n",
    "index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template = \"\"\"Use the chat messages (not sorted in any particular order) below to answer the given user query:\n",
    "    messages_list: {messages}\n",
    "    query: {query}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"messages\", \"query\"])\n",
    "llm = OpenAIChat(temperature=0)\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_results(query, limit=50):\n",
    "    query_emb = generator.encode_messages(query)\n",
    "\n",
    "    results = index.query(\n",
    "      vector=query_emb.tolist(),\n",
    "      top_k=limit,\n",
    "      include_values=False,\n",
    "      include_metadata=True\n",
    "    )\n",
    "\n",
    "    messages = []\n",
    "    for item in results[\"matches\"]:\n",
    "        # print(f\"\\nscore {item['score']}\")\n",
    "        # print(item[\"metadata\"][\"clean_message\"])\n",
    "        messages.append(item[\"metadata\"][\"clean_message\"])\n",
    "\n",
    "    return messages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is good about this project?\"\n",
    "messages = search_results(query)\n",
    "inputs = [{\"message\": msg} for _, msg in zip(range(len(messages)), messages)]\n",
    "result = chain.run({\"messages\":inputs, \"query\":query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what do users complain about this project?\"\n",
    "messages = search_results(query)\n",
    "inputs = [{\"index\": i, \"message\": msg} for i, msg in zip(range(len(messages)), messages)]\n",
    "result = chain.run({\"messages\":inputs, \"query\":query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# How to create a client with reset allowed\n",
    "# client = chromadb.HttpClient(settings=Settings(allow_reset=True))\n",
    "# client.reset()  # resets the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load telegram messages\n",
    "channel_id = \"@energyweb\"\n",
    "df = pd.read_csv(f\"notebooks/data/{channel_id}.csv\")\n",
    "\n",
    "df_loader = DataFrameLoader(df, page_content_column=\"history_str\")\n",
    "docs = df_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    - User_5297034533: @a84765641 [6401503585] to ...\n",
       "1                              - User_1174003449: ðŸ‘‹ðŸ»ðŸ‘‹ðŸ»\n",
       "2    - User_1482244625: I cannot wait to spin up a ...\n",
       "3    - User_1132548662: @saoyem Samuel, do you know...\n",
       "Name: history_str, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"history_str\"].iloc[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "hf_embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# comput embeddings and load to chroma\n",
    "db = Chroma.from_documents(docs, hf_embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to query\n",
    "def search_db(db_client, query: str, top_k = 100):\n",
    "    docs = db_client.similarity_search(query, k=top_k)\n",
    "\n",
    "    # print results\n",
    "    for item in docs[:5]:\n",
    "        print(f\"\\n{item.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fosho\n",
      "\n",
      "seems like they 'RMI'  really FOMO'ing for this,  especially after reading the\n",
      "\n",
      "Hellyea\n",
      "\n",
      "saoyem Nice Francesco\n",
      "\n",
      "zealy\n"
     ]
    }
   ],
   "source": [
    "query = \"fomo\"\n",
    "search_db(db, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peristent Chroma client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to delete  Collection embeddings_collection_energyweb does not exist.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "\n",
    "CLEAR_COLLECTION= True\n",
    "\n",
    "if CLEAR_COLLECTION:\n",
    "    try:\n",
    "        persistent_client.delete_collection(f\"embeddings_collection_{channel_id[1:]}\")\n",
    "    except Exception as e:\n",
    "        print(\"unable to delete \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "client = Chroma.from_documents(\n",
    "    docs, \n",
    "    embedding_function, \n",
    "    client=persistent_client, \n",
    "    collection_name = f\"embeddings_collection_{channel_id[1:]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- User_1853540265: A coinbase listing would be bullish\n",
      "- User_6010276849: In a bear season as well this,  not sure there will be effect. Listing sometimes has been o do with the right timing as well. The hype is there, you list and boom up to the sky.\n",
      "\n",
      "- User_1460230397: We're thrilled to announce that the @xenergyweb Crowdloan on @Polkadot\n",
      " is now live! Join Energy Web in shaping the energy future and contribute.\n",
      "\n",
      "For more information about the Crowdloan, visit: https://crowdloan.energywebx.com\n",
      "\n",
      "Like and Retweet here: https://twitter.com/energywebx/status/1698820250643411030\n",
      "- User_5986732143: This crowdloan went so well!!! Cannot believe itâ€™s a bear market at all.\n",
      "\n",
      "- User_1618131036: Another week went by. Hopefully the crypto market will recover soon\n",
      "- User_5418407519: I hope soo , we have endure bearish for a long time .\n",
      "\n",
      "- User_530622263: We are fortunate to be aware of such a great project during the last months of the bear market. A good time to dollar cost average IMO.\n",
      "- User_6448580728: Total agree\n",
      "\n",
      "- User_5454456997: Why day by day price go down?\n",
      "- User_6010276849: Happening to all it's the bear season, the best you can do for yourself mate is to wait for the bull run\n"
     ]
    }
   ],
   "source": [
    "query = \"bearish\"\n",
    "search_db(client, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_id = \"@runonflux\"\n",
    "df = pd.read_csv(f\"notebooks/data/{channel_id}.csv\")\n",
    "df_loader = DataFrameLoader(df, page_content_column=\"clean_message\")\n",
    "docs = df_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "\n",
    "CLEAR_COLLECTION = True\n",
    "\n",
    "if CLEAR_COLLECTION:\n",
    "    try:\n",
    "        persistent_client.delete_collection(f\"openai_embeddings_{channel_id[1:]}\")\n",
    "    except Exception as e:\n",
    "        print(\"unable to delete \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embeddings_function = OpenAIEmbeddings()\n",
    "\n",
    "openai_chroma_client = Chroma.from_documents(\n",
    "    docs, \n",
    "    openai_embeddings_function, \n",
    "    client=persistent_client, \n",
    "    collection_name = f\"openai_embeddings_{channel_id[1:]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- in addition to securing the network you also provide a real world case for solving problems  eg genome sequencing, graphics rendering, etc...\n",
      "- upto you to find and research\n",
      "- Can you please write in simple words the usecases of flux coin ? Services which requires flux token ?\n",
      "- What are the $FLUX usecases?Let's find out httpstwitter.comHouseofChimerastatus1671902756808818690\n",
      "- It may help to \n",
      "- Harness the power of decentralisation Explore RunOnFlux use cases Let's read onhttpstwitter.comHouseofChimerastatus1709901477689364513\n",
      "- cycle through the different options and see what the issue could be\n",
      "- Usage is one of the key things that separates Flux from its competitors, show the below some love on twitter and help spread the word httpstwitter.comcryptoviumstatus1686329414940823552?s20\n",
      "- Anyone can apply to use their services for payments\n",
      "- bring the network to life\n"
     ]
    }
   ],
   "source": [
    "query = \"use cases\"\n",
    "search_db(openai_chroma_client, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=openai_chroma_client.as_retriever(search_type=\"mmr\", search_kwargs={'fetch_k': 30}), \n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are some use cases of flux?\"\n",
    "response = qa({\"query\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Some potential use cases for Flux could include:\\n\\n1. Real-time data processing and analytics: Flux is designed to handle large volumes of data and perform computations in real-time, making it suitable for applications that require fast data processing and analytics.\\n\\n2. Internet of Things (IoT) applications: Flux can be used to collect and process data from various IoT devices, enabling the creation of smart and connected systems.\\n\\n3. Financial services: Flux's real-time capabilities make it well-suited for financial applications, such as high-frequency trading, fraud detection, and risk analysis.\\n\\n4. E-commerce and recommendation systems: Flux can be used to process and analyze user data in real-time, allowing for personalized recommendations and targeted advertising.\\n\\n5. Social media analytics: Flux's ability to handle streaming data makes it a good choice for analyzing social media feeds in real-time, enabling sentiment analysis, trend detection, and social network analysis.\\n\\nThese are just a few examples, and the use cases for Flux can vary depending on the specific requirements of an application.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing persisted chroma collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3117"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the existing collection has documents\n",
    "channel_id = \"@runonflux\"\n",
    "collection_name = f\"openai_embeddings_{channel_id[1:]}\"\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_collection(collection_name)\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "\n",
    "openai_embeddings_function = OpenAIEmbeddings()\n",
    "\n",
    "openai_client = Chroma(\n",
    "    persist_directory=\"./chroma\", \n",
    "    collection_name=collection_name, \n",
    "    embedding_function=openai_embeddings_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- People love to Fud and create unneeded distractions\n",
      "- I hope so too and we all hope so. But let's say that seeing your deleted message doesn't do much pleasure. And I certainly didn't take the liberty of making fud.\n",
      "- lets hope for the best\n",
      "- it's up to date mate \n",
      "- I think that now come dump. \n",
      "- chaos trouble violence disorder destruction confusion havocfracas commotion\n",
      "- reddug akhtarg need to be more vigilant with fake admin tagging\n",
      "- reddug could I do you ?\n",
      "- Ill give it another go\n",
      "- this is probably the only thing holding the project back\n"
     ]
    }
   ],
   "source": [
    "query = \"fud\"\n",
    "docs = openai_client.similarity_search(query, k=20)\n",
    "# print results\n",
    "for item in docs[:10]:\n",
    "    print(f\"- {item.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "template= \"\"\"\n",
    "The context given are several messages retrieved from a chat about a blockchain project. Based on those messages try to answer the user question.\n",
    "----------------\n",
    "context: {context}\n",
    "user question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=openai_client.as_retriever(search_type=\"mmr\", search_kwargs={'fetch_k': 100}), \n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] template='\\nThe context given are several messages retrieved from a chat about a blockchain project. Based on those messages try to answer the user question.\\n----------------\\ncontext: {context}\\nuser question: {question}\\n'\n"
     ]
    }
   ],
   "source": [
    "print(qa.combine_documents_chain.llm_chain.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"What makes this project unique?\"\n",
    "response = qa({\"query\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the given context, it seems that the project mentioned (Flux) is being compared to other blockchain projects. One user mentions that the project is \"probably the only thing holding the project back.\" Another user asks what features make Flux stand out compared to other chains.\\n\\nHowever, there is not enough information provided in the given messages to determine what specifically makes Flux unique. Further details or messages are needed to answer the user\\'s question accurately.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"result\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram-nlp-6m6KizWy-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
