{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore semantic search on telegram messages\n",
    "# preprocess the data. e.g remove short texts\n",
    "# try indexing the data with pinecone to check the latency (5000 messages?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilo.ramirez/Library/Caches/pypoetry/virtualenvs/telegram-nlp-6m6KizWy-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "from telethon.sync import TelegramClient\n",
    "from IPython.display import display\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import pinecone\n",
    "\n",
    "from telethon.tl.types import InputMessagesFilterEmpty\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# do not truncate column width in pandas\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/camilo.ramirez/Documents/mai-repos/telegram-nlp'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/camilo.ramirez/Documents/mai-repos/telegram-nlp/notebooks',\n",
       " '/Users/camilo.ramirez/Documents/mai-repos/telegram-nlp/notebooks',\n",
       " '//modules',\n",
       " '/Users/camilo.ramirez/.pyenv/versions/3.10.11/lib/python310.zip',\n",
       " '/Users/camilo.ramirez/.pyenv/versions/3.10.11/lib/python3.10',\n",
       " '/Users/camilo.ramirez/.pyenv/versions/3.10.11/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/Users/camilo.ramirez/Library/Caches/pypoetry/virtualenvs/telegram-nlp-6m6KizWy-py3.10/lib/python3.10/site-packages']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = \"/Users/camilo.ramirez/Documents/mai-repos/telegram-nl/modules\"\n",
    "sys.path.insert(0, foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/camilo.ramirez/Documents/mai-repos/telegram-nl/modules',\n",
       " '/Users/camilo.ramirez/Documents/mai-repos/telegram-nlp/notebooks',\n",
       " '/Users/camilo.ramirez/Documents/mai-repos/telegram-nlp/notebooks',\n",
       " '//modules',\n",
       " '/Users/camilo.ramirez/.pyenv/versions/3.10.11/lib/python310.zip',\n",
       " '/Users/camilo.ramirez/.pyenv/versions/3.10.11/lib/python3.10',\n",
       " '/Users/camilo.ramirez/.pyenv/versions/3.10.11/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/Users/camilo.ramirez/Library/Caches/pypoetry/virtualenvs/telegram-nlp-6m6KizWy-py3.10/lib/python3.10/site-packages']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'://modules/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PYTHONPATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_dir = os.path.split(os.getcwd())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/camilo.ramirez/Documents/mai-repos/telegram-nlp', 'notebooks')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.split(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/camilo.ramirez/Documents/mai-repos/telegram-nlp/notebooks'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/camilo.ramirez/Documents/mai-repos/telegram-nlp'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/camilo.ramirez/Documents/mai-repos/telegram-nlp/notebooks/telegram_semantic_search.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/camilo.ramirez/Documents/mai-repos/telegram-nlp/notebooks/telegram_semantic_search.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msentiment_predictor\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'modules'"
     ]
    }
   ],
   "source": [
    "import modules.sentiment_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_id = os.environ[\"TELEGRAM_API_ID\"]\n",
    "api_hash = os.environ[\"TELEGRAM_API_HASH\"]\n",
    "pinecone_key = os.environ[\"PINECONE_APIKEY\"]\n",
    "phone = \"+34634454832\"\n",
    "username = \"@elvesipeto\"\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = []\n",
    "\n",
    "columns = [\"channel_name\", \"id\", \"peer_id\", \"date\", \"message\", \"out\", \"mentioned\",\n",
    "        \"media_unread\", \"silent\", \"post\", \"from_scheduled\", \"legacy\", \n",
    "        \"edit_hide\", \"pinned\",\"noforwards\", \"from_id\", \"fwd_from\", \"via_bot_id\",\n",
    "        \"reply_to\", \"media\", \"reply_markup\", \"entities\", \"views\",\n",
    "        \"forwards\", \"replies\", \"edit_date\", \"post_author\", \"grouped_id\",\n",
    "        \"reactions\", \"restriction_reason\", \"ttl_period\"]\n",
    "\n",
    "client = TelegramClient(\"session_temp\", api_id, api_hash)\n",
    "channel_id = \"@runonflux\"\n",
    "n = 5000\n",
    "\n",
    "async with client:        \n",
    "    async for msg in client.iter_messages(channel_id, filter=InputMessagesFilterEmpty, limit=n):\n",
    "        pd_data.append((channel_id, msg.id,msg.peer_id, msg.date, msg.message,\n",
    "                msg.out, msg.mentioned, msg.media_unread, msg.silent,msg.post,\n",
    "                msg.from_scheduled, msg.legacy, msg.edit_hide, msg.pinned, msg.noforwards,\n",
    "                msg.from_id, msg.fwd_from, msg.via_bot_id, msg.reply_to, msg.media, msg.reply_markup,\n",
    "                msg.entities, msg.views, msg.forwards, msg.replies, msg.edit_date, msg.post_author,\n",
    "                msg.grouped_id, msg.reactions, msg.restriction_reason, msg.ttl_period\n",
    "        ))\n",
    "\n",
    "df = pd.DataFrame(pd_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total message count 5000\n",
      "Total messages after excluding empty 4849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10      1.0\n",
       "0.25      4.0\n",
       "0.50      9.0\n",
       "0.75     19.0\n",
       "0.95     57.6\n",
       "0.98    135.0\n",
       "Name: token_count, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total messages after excluding long and short messages 3462\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and filtering \n",
    "\n",
    "df[\"token_count\"] = df[\"message\"].apply(lambda x: len(x.split(\" \"))  if type(x) == str else 0)\n",
    "print(\"Total message count\", len(df))\n",
    "\n",
    "filtered_df = df[~df[\"message\"].isna()]\n",
    "print(\"Total messages after excluding empty\", len(filtered_df))\n",
    "\n",
    "# What is the distribution of token counts?\n",
    "display(filtered_df[\"token_count\"].quantile([.1, .25, .5, .75, .95, 0.98]))\n",
    "\n",
    "\n",
    "lower_limit = 4\n",
    "\n",
    "# 250 was the max input lenght of the training data for multi-qa-MiniLM-L6-cos-v1\n",
    "upper_limit = 250 \n",
    "\n",
    "filtered_df = filtered_df[(filtered_df[\"token_count\"] > lower_limit) & (filtered_df[\"token_count\"] < upper_limit)]\n",
    "print(\"Total messages after excluding long and short messages\", len(filtered_df))\n",
    "\n",
    "# clean messages\n",
    "filtered_df.loc[:, \"clean_message\"] = filtered_df[\"message\"].apply(lambda x: re.sub('[^A-Za-z0-9 .,?$%\\'\"]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(f\"data/{channel_id}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(\n",
    "    api_key=os.environ[\"PINECONE_APIKEY\"],\n",
    "    environment=\"us-west1-gcp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings model\n",
    "multi_qa_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "query = \"This coin will moon soon\"\n",
    "vec = multi_qa_encoder.encode(query, convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_DELETE_INDEX = True\n",
    "INDEX_NAME = \"telegram-embeddings\"\n",
    "\n",
    "if FORCE_DELETE_INDEX:\n",
    "    pinecone.delete_index(INDEX_NAME)\n",
    "\n",
    "if INDEX_NAME not in pinecone.list_indexes():\n",
    "    pinecone.create_index(INDEX_NAME, dimension=len(vec))\n",
    "    \n",
    "# connect to index\n",
    "index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    has_gpu = torch.cuda.is_available()\n",
    "    has_mps = torch.backends.mps.is_built()\n",
    "    device = \"mps\" if has_mps else \"gpu\" if has_gpu else \"cpu\"\n",
    "    return device\n",
    "\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>id</th>\n",
       "      <th>peer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>message</th>\n",
       "      <th>out</th>\n",
       "      <th>mentioned</th>\n",
       "      <th>media_unread</th>\n",
       "      <th>silent</th>\n",
       "      <th>post</th>\n",
       "      <th>from_scheduled</th>\n",
       "      <th>legacy</th>\n",
       "      <th>edit_hide</th>\n",
       "      <th>pinned</th>\n",
       "      <th>noforwards</th>\n",
       "      <th>from_id</th>\n",
       "      <th>fwd_from</th>\n",
       "      <th>via_bot_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>media</th>\n",
       "      <th>reply_markup</th>\n",
       "      <th>entities</th>\n",
       "      <th>views</th>\n",
       "      <th>forwards</th>\n",
       "      <th>replies</th>\n",
       "      <th>edit_date</th>\n",
       "      <th>post_author</th>\n",
       "      <th>grouped_id</th>\n",
       "      <th>reactions</th>\n",
       "      <th>restriction_reason</th>\n",
       "      <th>ttl_period</th>\n",
       "      <th>token_count</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@runonflux</td>\n",
       "      <td>492112</td>\n",
       "      <td>PeerChannel(channel_id=1202951585)</td>\n",
       "      <td>2023-10-09 15:03:45+00:00</td>\n",
       "      <td>I agree. A level around $0.45/$0.50 would be a more bullish sign. Considering that at the beginning of the year this was also more or less the price</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>PeerUser(user_id=1879556613)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MessageReplyHeader(reply_to_msg_id=492105, reply_to_scheduled=False, forum_topic=True, reply_to_peer_id=None, reply_to_top_id=476033)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>I agree. A level around $0.45$0.50 would be a more bullish sign. Considering that at the beginning of the year this was also more or less the price</td>\n",
       "      <td>[tensor(-0.0158, device='mps:0'), tensor(0.0359, device='mps:0'), tensor(-0.0107, device='mps:0'), tensor(0.0137, device='mps:0'), tensor(-0.0404, device='mps:0'), tensor(-0.0157, device='mps:0'),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  channel_name      id                             peer_id  \\\n",
       "0   @runonflux  492112  PeerChannel(channel_id=1202951585)   \n",
       "\n",
       "                        date  \\\n",
       "0  2023-10-09 15:03:45+00:00   \n",
       "\n",
       "                                                                                                                                                message  \\\n",
       "0  I agree. A level around $0.45/$0.50 would be a more bullish sign. Considering that at the beginning of the year this was also more or less the price   \n",
       "\n",
       "     out  mentioned  media_unread  silent   post  from_scheduled  legacy  \\\n",
       "0  False      False         False   False  False           False   False   \n",
       "\n",
       "   edit_hide  pinned  noforwards                       from_id fwd_from  \\\n",
       "0      False   False       False  PeerUser(user_id=1879556613)      NaN   \n",
       "\n",
       "   via_bot_id  \\\n",
       "0         NaN   \n",
       "\n",
       "                                                                                                                                reply_to  \\\n",
       "0  MessageReplyHeader(reply_to_msg_id=492105, reply_to_scheduled=False, forum_topic=True, reply_to_peer_id=None, reply_to_top_id=476033)   \n",
       "\n",
       "  media reply_markup entities  views  forwards replies edit_date  post_author  \\\n",
       "0   NaN          NaN      NaN    NaN       NaN     NaN       NaN          NaN   \n",
       "\n",
       "   grouped_id reactions  restriction_reason  ttl_period  token_count  \\\n",
       "0         NaN       NaN                 NaN         NaN           28   \n",
       "\n",
       "                                                                                                                                         clean_message  \\\n",
       "0  I agree. A level around $0.45$0.50 would be a more bullish sign. Considering that at the beginning of the year this was also more or less the price   \n",
       "\n",
       "                                                                                                                                                                                                embeddings  \n",
       "0  [tensor(-0.0158, device='mps:0'), tensor(0.0359, device='mps:0'), tensor(-0.0107, device='mps:0'), tensor(0.0137, device='mps:0'), tensor(-0.0404, device='mps:0'), tensor(-0.0157, device='mps:0'),...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload data to index\n",
    "\n",
    "COMPUTE_EMBEDDINGS = True\n",
    "if COMPUTE_EMBEDDINGS:\n",
    "    # preprocess data\n",
    "    df = pd.read_csv(f\"data/{channel_id}.csv\")\n",
    "\n",
    "    # create embeddings\n",
    "    df[\"embeddings\"] = df[\"clean_message\"].apply(lambda x: multi_qa_encoder.encode(x, device=\"mps\", convert_to_tensor=True, show_progress_bar=False))\n",
    "    \n",
    "    index_upsert = []\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try encoding with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "telegram_messages = df[\"clean_message\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8341584205627441 A node is more profitable i guess\n",
      "0.8178715705871582 How much are nodes still profitable?\n",
      "0.779725193977356 oh yeah sorry I didn't mean nodes are not any more profitable\n",
      "0.7323274612426758 Still profitable. 70$ cost, 140$ rewards.Nobody is saying that it wasn't more at some point, it was also less at some point.But nodes are profitable\n",
      "0.6950095891952515 If some nodes close down, the rest becomes more profitable. It equals itself out.\n",
      "0.6212084889411926 That was an answer to a different question.Also nodes are still profitable. My stratus costs around 70$ to run and gets about 130$ in rewards. Without future pa\n",
      "0.5766648054122925 its strict benchmark, not all average person can run a node.\n",
      "0.5579705238342285 Margins are tight but still profitable if you run it from home and already pay for internet regardless of nodes\n",
      "0.5410272479057312 your setup much be inefficient  my nodes are profitable\n",
      "0.5327602028846741 But I see I should make research what is most profitable\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take average of all tokens\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "#Encode text\n",
    "def encode(texts, device=\"cpu\"):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(\n",
    "        texts,\n",
    "        max_length=64,\n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        return_tensors='pt')\n",
    "    \n",
    "    \n",
    "\n",
    "    input_ids = encoded_input[\"input_ids\"].to(device)\n",
    "    attention_masks = encoded_input[\"attention_mask\"].to(device)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(outputs, attention_mask=attention_masks)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "query = \"What is the most profitable node?\"\n",
    "docs = telegram_messages\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "#Encode query and docs\n",
    "query_emb = encode(query, device=\"cpu\")\n",
    "doc_emb = encode(docs, device=\"cpu\")\n",
    "\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs[:10]:\n",
    "    print(score, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9156372547149658 Around 9 Million people live in London\n",
      "0.4947578012943268 London is known for its financial district\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take average of all tokens\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "query = \"How many people live in London?\"\n",
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "#Encode query and docs\n",
    "query_emb = encode(query)\n",
    "doc_emb = encode(docs)\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs:\n",
    "    print(score, doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload eembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating messages 0-1000\n",
      "inserting batch 999\n",
      "iterating messages 1000-2000\n",
      "inserting batch 1999\n",
      "iterating messages 2000-3000\n",
      "inserting batch 2999\n",
      "iterating messages 3000-4000\n",
      "inserting batch 3461\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "UPLOAD_VECTORS = True\n",
    "if UPLOAD_VECTORS:    \n",
    "    batch_size = 1000\n",
    "    total_batches = math.ceil(len(df) / batch_size)\n",
    "    start = 0\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        index_upsert = [] # always initialize for each batch        \n",
    "        end = start + batch_size\n",
    "\n",
    "        print(f\"iterating messages {start}-{end}\")\n",
    "        for i, item in df[start:end].iterrows():\n",
    "            index_upsert += [\n",
    "                    (str(i), \n",
    "                    item[\"embeddings\"].tolist(),\n",
    "                    {\n",
    "                        \"clean_message\": item[\"clean_message\"],\n",
    "                        \"channel_name\": item[\"channel_name\"],\n",
    "                        \"messagee_id\": item[\"id\"]\n",
    "                    })\n",
    "            ]\n",
    "        start = end\n",
    "        print(f\"inserting batch {i}\")\n",
    "        index.upsert(vectors=index_upsert) # can contain maximum 1000 items        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-playground-K_szDW0O-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
