{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic search on telegram channels\n",
    "\n",
    "Showcasing how to extract telegram messages for a specific channel and perform semantic search given an user query. \n",
    "I use pretrained models for semantic search from huggingface.\n",
    "\n",
    "The fetched messages are first embbeded with the model and then uploaded to a pinecode index. Then the query is embedded and compared against the indexed data.\n",
    "\n",
    "An option for future work is to fine tune a model specific for telegram messages of certain topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilo.ramirez/Library/Caches/pypoetry/virtualenvs/telegram-nlp-6m6KizWy-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "from telethon.sync import TelegramClient\n",
    "from IPython.display import display\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import pinecone\n",
    "\n",
    "from telethon.tl.types import InputMessagesFilterEmpty\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# do not truncate column width in pandas\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_id = os.environ[\"TELEGRAM_API_ID\"]\n",
    "api_hash = os.environ[\"TELEGRAM_API_HASH\"]\n",
    "pinecone_key = os.environ[\"PINECONE_APIKEY\"]\n",
    "phone = \"+34634454832\"\n",
    "username = \"@elvesipeto\"\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fetching and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = []\n",
    "\n",
    "columns = [\"channel_name\", \"id\", \"peer_id\", \"date\", \"message\", \"out\", \"mentioned\",\n",
    "        \"media_unread\", \"silent\", \"post\", \"from_scheduled\", \"legacy\", \n",
    "        \"edit_hide\", \"pinned\",\"noforwards\", \"from_id\", \"fwd_from\", \"via_bot_id\",\n",
    "        \"reply_to\", \"media\", \"reply_markup\", \"entities\", \"views\",\n",
    "        \"forwards\", \"replies\", \"edit_date\", \"post_author\", \"grouped_id\",\n",
    "        \"reactions\", \"restriction_reason\", \"ttl_period\"]\n",
    "\n",
    "client = TelegramClient(f\"sessions_data/{phone}\", api_id, api_hash)\n",
    "channel_id = \"@runonflux\"\n",
    "n = 5000\n",
    "\n",
    "async with client:        \n",
    "    async for msg in client.iter_messages(channel_id, filter=InputMessagesFilterEmpty, limit=n):\n",
    "        pd_data.append((channel_id, msg.id,msg.peer_id, msg.date, msg.message,\n",
    "                msg.out, msg.mentioned, msg.media_unread, msg.silent,msg.post,\n",
    "                msg.from_scheduled, msg.legacy, msg.edit_hide, msg.pinned, msg.noforwards,\n",
    "                msg.from_id, msg.fwd_from, msg.via_bot_id, msg.reply_to, msg.media, msg.reply_markup,\n",
    "                msg.entities, msg.views, msg.forwards, msg.replies, msg.edit_date, msg.post_author,\n",
    "                msg.grouped_id, msg.reactions, msg.restriction_reason, msg.ttl_period\n",
    "        ))\n",
    "\n",
    "df = pd.DataFrame(pd_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total message count 5000\n",
      "Total messages after excluding empty 4848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10      1.00\n",
       "0.25      4.00\n",
       "0.50      9.00\n",
       "0.75     19.00\n",
       "0.95     56.00\n",
       "0.98    132.06\n",
       "Name: token_count, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total messages after excluding long and short messages 3459\n",
      "Total messages after excluding duplicates 3117\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and filtering \n",
    "\n",
    "df[\"token_count\"] = df[\"message\"].apply(lambda x: len(x.split(\" \"))  if type(x) == str else 0)\n",
    "print(\"Total message count\", len(df))\n",
    "\n",
    "filtered_df = df[~df[\"message\"].isna()]\n",
    "print(\"Total messages after excluding empty\", len(filtered_df))\n",
    "\n",
    "# What is the distribution of token counts?\n",
    "display(filtered_df[\"token_count\"].quantile([.1, .25, .5, .75, .95, 0.98]))\n",
    "\n",
    "\n",
    "lower_limit = 4\n",
    "\n",
    "# 250 was the max input lenght of the training data for multi-qa-MiniLM-L6-cos-v1\n",
    "upper_limit = 250 \n",
    "\n",
    "filtered_df = filtered_df[(filtered_df[\"token_count\"] > lower_limit) & (filtered_df[\"token_count\"] < upper_limit)]\n",
    "print(\"Total messages after excluding long and short messages\", len(filtered_df))\n",
    "\n",
    "# clean messages\n",
    "filtered_df.loc[:, \"clean_message\"] = filtered_df[\"message\"].apply(lambda x: re.sub('[^A-Za-z0-9 .,?$%\\'\"]+', '', x))\n",
    "\n",
    "# remove duplicates\n",
    "filtered_df = filtered_df[~filtered_df.duplicated(subset=[\"clean_message\"])]\n",
    "print(\"Total messages after excluding duplicates\", len(filtered_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(f\"notebooks/data/{channel_id}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from semantic_search_generator import SemanticSearchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This flux 4 upgrade made 6 of my nodes go offline.  Now is dos mode.How to get out if this please?',\n",
       " 'I would suggest you to stop promoting the group BettyK0',\n",
       " 'Yes deleting nodes because of low rewards is the most scary thing  we may even see less than 0.20 ',\n",
       " 'Join our CBO Davy Wittock for a special mining episode for Around the Blockchain today at 5 PM EST Get ready for an exciting discussion about Proof of Useful Work and the future of mining Link  httpsyoutube.comaroundtheblockchainofficial',\n",
       " ' many crypto projects are one sided in their community. Most are in discord, some are almost exclusively on telegram, some loosely on Twitter....',\n",
       " \"My buy in was 60c but I asked literally everyone big in the flux space if we could maintain the dollar and they were all bullish including Dan. I wanted to sell to buy back cheaper and am kicking myself I didn't lol\",\n",
       " \"What's command on TG Flux  to check amount stake titan node ?\",\n",
       " 'For example, my stratus gives me 110 a month with current price and APR. Cost of the dedicated server is 70.If i wanted to run cumulus instead, it would be 40 cumulus, giving me about 220 total a month, but costing me about 210 in VPS cost.Plus now i have to monitor and maintain 40 nodes instead of one ',\n",
       " 'Anybody withdraw Flux  from CoinEX?Looks suspended',\n",
       " 'We only have 400mil flux  was it an amount allocated for this parallel claims ?']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_id = \"@runonflux\"\n",
    "df = pd.read_csv(f\"notebooks/data/{channel_id}.csv\")\n",
    "messages = df.sample(10, random_state=11)[\"clean_message\"].to_list()\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2212686538696289 My buy in was 60c but I asked literally everyone big in the flux space if we could maintain the dollar and they were all bullish including Dan. I wanted to sell to buy back cheaper and am kicking myself I didn't lol\n",
      "0.10311347246170044 For example, my stratus gives me 110 a month with current price and APR. Cost of the dedicated server is 70.If i wanted to run cumulus instead, it would be 40 cumulus, giving me about 220 total a month, but costing me about 210 in VPS cost.Plus now i have to monitor and maintain 40 nodes instead of one \n",
      "0.09914617985486984 Join our CBO Davy Wittock for a special mining episode for Around the Blockchain today at 5 PM EST Get ready for an exciting discussion about Proof of Useful Work and the future of mining Link  httpsyoutube.comaroundtheblockchainofficial\n",
      "0.08931327611207962 I would suggest you to stop promoting the group BettyK0\n",
      "0.06712281703948975 Anybody withdraw Flux  from CoinEX?Looks suspended\n",
      "0.04982435703277588  many crypto projects are one sided in their community. Most are in discord, some are almost exclusively on telegram, some loosely on Twitter....\n",
      "0.04696136713027954 Yes deleting nodes because of low rewards is the most scary thing  we may even see less than 0.20 \n",
      "-0.005718698725104332 We only have 400mil flux  was it an amount allocated for this parallel claims ?\n",
      "-0.024313876405358315 What's command on TG Flux  to check amount stake titan node ?\n",
      "-0.048260483890771866 This flux 4 upgrade made 6 of my nodes go offline.  Now is dos mode.How to get out if this please?\n"
     ]
    }
   ],
   "source": [
    "generator = SemanticSearchGenerator()\n",
    "\n",
    "# Text to encode\n",
    "query = \"investment strategy\"\n",
    "doc_score_pairs = generator.search_batch(query, messages, device=\"mps\")\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs[:10]:\n",
    "    print(score, doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_id = \"@runonflux\"\n",
    "model_name = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "df = pd.read_csv(f\"notebooks/data/{channel_id}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(\n",
    "    api_key=os.environ[\"PINECONE_APIKEY\"],\n",
    "    environment=\"us-west1-gcp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings model\n",
    "multi_qa_encoder = SentenceTransformer(model_name)\n",
    "\n",
    "query = \"This coin will moon soon\"\n",
    "vec = multi_qa_encoder.encode(query, convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_DELETE_INDEX = False # Set True only for initializing the index\n",
    "INDEX_NAME = \"telegram-embeddings\"\n",
    "\n",
    "if FORCE_DELETE_INDEX:\n",
    "    pinecone.delete_index(INDEX_NAME)\n",
    "\n",
    "if INDEX_NAME not in pinecone.list_indexes():\n",
    "    pinecone.create_index(INDEX_NAME, dimension=len(vec))\n",
    "    \n",
    "# connect to index\n",
    "index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    has_gpu = torch.cuda.is_available()\n",
    "    has_mps = torch.backends.mps.is_built()\n",
    "    device = \"mps\" if has_mps else \"gpu\" if has_gpu else \"cpu\"\n",
    "    return device\n",
    "\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to index\n",
    "COMPUTE_EMBEDDINGS = True\n",
    "if COMPUTE_EMBEDDINGS:\n",
    "    # create embeddings\n",
    "    df[\"embeddings_cpu\"] = df[\"clean_message\"].apply(lambda x: multi_qa_encoder.encode(x, device=\"cpu\", convert_to_tensor=True, show_progress_bar=False))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = SemanticSearchGenerator(model_name)\n",
    "\n",
    "# Faster with mps\n",
    "df[\"embeddings_mps\"] = df[\"clean_message\"].apply(lambda x: generator.encode_messages(x, device=\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embeddings_cpu</th>\n",
       "      <th>embeddings_mps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[tensor(-0.0749), tensor(-0.1107), tensor(-0.0339), tensor(0.0374), tensor(-0.0197), tensor(0.0569), tensor(0.0786), tensor(-0.0522), tensor(-0.0159), tensor(0.0207), tensor(-0.0155), tensor(-0.01...</td>\n",
       "      <td>[[tensor(-0.0749, device='mps:0'), tensor(-0.1107, device='mps:0'), tensor(-0.0339, device='mps:0'), tensor(0.0374, device='mps:0'), tensor(-0.0197, device='mps:0'), tensor(0.0569, device='mps:0')...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tensor(0.0309), tensor(-0.0022), tensor(0.0416), tensor(0.0120), tensor(0.0061), tensor(0.0405), tensor(-0.1566), tensor(-0.0018), tensor(-0.1030), tensor(0.0587), tensor(0.0292), tensor(0.0281),...</td>\n",
       "      <td>[[tensor(0.0309, device='mps:0'), tensor(-0.0022, device='mps:0'), tensor(0.0416, device='mps:0'), tensor(0.0120, device='mps:0'), tensor(0.0061, device='mps:0'), tensor(0.0405, device='mps:0'), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tensor(0.0337), tensor(-0.0346), tensor(-0.0315), tensor(-0.0759), tensor(-0.0189), tensor(0.0352), tensor(-0.0644), tensor(0.0090), tensor(-0.0890), tensor(-0.0099), tensor(-0.1018), tensor(-0.0...</td>\n",
       "      <td>[[tensor(0.0337, device='mps:0'), tensor(-0.0346, device='mps:0'), tensor(-0.0315, device='mps:0'), tensor(-0.0759, device='mps:0'), tensor(-0.0189, device='mps:0'), tensor(0.0352, device='mps:0')...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                            embeddings_cpu  \\\n",
       "0  [tensor(-0.0749), tensor(-0.1107), tensor(-0.0339), tensor(0.0374), tensor(-0.0197), tensor(0.0569), tensor(0.0786), tensor(-0.0522), tensor(-0.0159), tensor(0.0207), tensor(-0.0155), tensor(-0.01...   \n",
       "1  [tensor(0.0309), tensor(-0.0022), tensor(0.0416), tensor(0.0120), tensor(0.0061), tensor(0.0405), tensor(-0.1566), tensor(-0.0018), tensor(-0.1030), tensor(0.0587), tensor(0.0292), tensor(0.0281),...   \n",
       "2  [tensor(0.0337), tensor(-0.0346), tensor(-0.0315), tensor(-0.0759), tensor(-0.0189), tensor(0.0352), tensor(-0.0644), tensor(0.0090), tensor(-0.0890), tensor(-0.0099), tensor(-0.1018), tensor(-0.0...   \n",
       "\n",
       "                                                                                                                                                                                            embeddings_mps  \n",
       "0  [[tensor(-0.0749, device='mps:0'), tensor(-0.1107, device='mps:0'), tensor(-0.0339, device='mps:0'), tensor(0.0374, device='mps:0'), tensor(-0.0197, device='mps:0'), tensor(0.0569, device='mps:0')...  \n",
       "1  [[tensor(0.0309, device='mps:0'), tensor(-0.0022, device='mps:0'), tensor(0.0416, device='mps:0'), tensor(0.0120, device='mps:0'), tensor(0.0061, device='mps:0'), tensor(0.0405, device='mps:0'), t...  \n",
       "2  [[tensor(0.0337, device='mps:0'), tensor(-0.0346, device='mps:0'), tensor(-0.0315, device='mps:0'), tensor(-0.0759, device='mps:0'), tensor(-0.0189, device='mps:0'), tensor(0.0352, device='mps:0')...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"embeddings_cpu\", \"embeddings_mps\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "UPLOAD_VECTORS = False # only for index initialization\n",
    "if UPLOAD_VECTORS:    \n",
    "    batch_size = 1000\n",
    "    total_batches = math.ceil(len(df) / batch_size)\n",
    "    start = 0\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        index_upsert = [] # always initialize for each batch        \n",
    "        end = start + batch_size\n",
    "\n",
    "        print(f\"iterating messages {start}-{end}\")\n",
    "        for j, item in df[start:end].iterrows():\n",
    "            index_upsert += [\n",
    "                    (str(j), \n",
    "                    item[\"embeddings_cpu\"].tolist(),\n",
    "                    {\n",
    "                        \"clean_message\": item[\"clean_message\"],\n",
    "                        \"channel_name\": item[\"channel_name\"],\n",
    "                        \"messagee_id\": item[\"id\"]\n",
    "                    })\n",
    "            ]\n",
    "        start = end\n",
    "        print(f\"inserting batch {i}\")\n",
    "        index.upsert(vectors=index_upsert) # can contain maximum 1000 items        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search on index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "score 0.410763919\n",
      "Typical bear market... Engagement drops, people hesitate. Meanwhile the team continues to build, POUW just presented a couple of weeks ago\n",
      "\n",
      "score 0.401366264\n",
      "keep an eye on announcements\n",
      "\n",
      "score 0.387041956\n",
      "Nothing happened, bear markets are a bitch\n",
      "\n",
      "score 0.362539232\n",
      "If you interested list in our exchange I can help you\n",
      "\n",
      "score 0.350785732\n",
      "Bearmarkets dont last forever. Unless you are in a scam project. I dont see any other thing here than non stopping hardwork. So chill \n"
     ]
    }
   ],
   "source": [
    "query = \"bearish outlook\"\n",
    "query_emb = generator.encode_messages(query)\n",
    "\n",
    "results = index.query(\n",
    "  vector=query_emb.tolist(),\n",
    "  top_k=10,\n",
    "  include_values=False,\n",
    "  include_metadata=True\n",
    ")\n",
    "\n",
    "for item in results[\"matches\"][:5]:\n",
    "    print(f\"\\nscore {item['score']}\")\n",
    "    print(item[\"metadata\"][\"clean_message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "score 0.501404643\n",
      "Hey here A beautiful week to all, its amazing how when the market is bearing or in halt the vibes in communities change, you can see clearly the vision and the rational unlike in bull where everyone is jumping in without research\n",
      "\n",
      "score 0.4879556\n",
      "I've never been a fan of the bullish halving narrative, to many disappointed people who fell for the hype.\n",
      "\n",
      "score 0.407264948\n",
      "Who said anything about expecting a bull run ? I said i will cash out my flux when it hits 1$ again\n",
      "\n",
      "score 0.40503487\n",
      "Is there a bot's news function or roadmap?\n",
      "\n",
      "score 0.404052913\n",
      "I know what you mean but in bull it will be worthed\n"
     ]
    }
   ],
   "source": [
    "query = \"bullish news\"\n",
    "query_emb = generator.encode_messages(query)\n",
    "\n",
    "results = index.query(\n",
    "  vector=query_emb.tolist(),\n",
    "  top_k=10,\n",
    "  include_values=False,\n",
    "  include_metadata=True\n",
    ")\n",
    "\n",
    "for item in results[\"matches\"][:5]:\n",
    "    print(f\"\\nscore {item['score']}\")\n",
    "    print(item[\"metadata\"][\"clean_message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "score 0.420437932\n",
      "In Q3 of 2023, we plan to launch the proof of concept for Proof of Useful Work.We are excited to partner with Octominer to collaborate on finding optimal builds for PoUW Learn more about the strategic partnership at httpsfluxofficial.medium.comfluxpartnerswithoctominer884353d75854\n",
      "\n",
      "score 0.409998924\n",
      "proof of concept is like the first iteration  will it work the way its planned  are there any changes needed  testing the concept is good\n",
      "\n",
      "score 0.396739244\n",
      "Discover how Proof of Useful Work solves Web2 cloud infrastructure problems and unlocks new possibilities for AI and more. Check out our latest article httpsfluxofficial.medium.comfluxproofofusefulworkablockchainmilestone6ba11d7d52f5\n",
      "\n",
      "score 0.380016953\n",
      "Hi $FLUX communityWhat is Proof of Useful Work PoUW by Run on Flux?Let's read onhttpstwitter.comHouseofChimerastatus1694010279544136181\n",
      "\n",
      "score 0.371297866\n",
      "Catch the RunOnFlux Proof of Useful Work Announcement today at CypherPunk2023 at 1015am EST LIVE.Watch LIVE httpsyoutube.comliveOXnsbSUOD8?featureshare\n"
     ]
    }
   ],
   "source": [
    "query = \"proof of useful work\"\n",
    "query_emb = generator.encode_messages(query)\n",
    "\n",
    "results = index.query(\n",
    "  vector=query_emb.tolist(),\n",
    "  top_k=10,\n",
    "  include_values=False,\n",
    "  include_metadata=True\n",
    ")\n",
    "\n",
    "for item in results[\"matches\"][:5]:\n",
    "    print(f\"\\nscore {item['score']}\")\n",
    "    print(item[\"metadata\"][\"clean_message\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-playground-K_szDW0O-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
